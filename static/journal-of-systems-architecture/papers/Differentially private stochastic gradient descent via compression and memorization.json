{
    "url": "https://www.sciencedirect.com/science/article/pii/S1383762122003046",
    "title": "Differentially private stochastic gradient descent via compression and memorization",
    "abstract": "We propose a novel approach for achieving differential privacy for  neural network  training models through compression and memorization of gradients. The compression technique, which makes gradient vectors sparse, reduces the sensitivity so that differential privacy can be achieved with less noise; whereas the memorization technique, which remembers unused gradient parts, keeps track of the descent direction and thereby maintains the accuracy of the proposed algorithm. Our differentially private algorithm, called  dp-memSGD  for short, converges mathematically at the same rate of  1 / T  as standard  stochastic gradient descent  (SGD) algorithm, where  T  is the number of training iterations. Experimentally, we demonstrate that  dp-memSGD  converges with reasonable privacy losses on many benchmark datasets.",
    "citation_count": "12",
    "year": "2023/02/01",
    "authors": [
        {
            "name": "Le Trieu Phong",
            "country": ""
        },
        {
            "name": "Tran Thi Phuong",
            "country": ""
        }
    ],
    "keywords": []
}