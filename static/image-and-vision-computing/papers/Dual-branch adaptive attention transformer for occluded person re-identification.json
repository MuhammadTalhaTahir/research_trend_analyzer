{
    "url": "https://www.sciencedirect.com/science/article/pii/S0262885623000070",
    "title": "Dual-branch adaptive attention transformer for occluded person re-identification",
    "abstract": "Occluded person re-identification is still a common and challenging task because people are often occluded by some obstacles (e.g. cars and trees) in the real world. In order to locate the unoccluded parts and extract local fine-grained features of the occluded human body, State-of-the-Art (SOTA) methods usually use a pose estimation model, which usually causes additional bias and this two-stage architecture also complicates the model. To solve this problem, an end-to-end dual-branch Transformer network for occluded person re-identification is designed. Specifically, one of the branches is the transformer-based global branch, which is responsible for extracting global features, while in the other local branch, we design the Selective Token Attention (STA) module. STA can utilize the multi-headed self-attention mechanism to select discriminating tokens for effectively extracting the local features. Further, in order to alleviate the inconsistency between Softmax Loss and Triplet Loss convergence goals, Circle Loss is introduced to design the Goal Consistency Loss (GC Loss) to supervise the network. Experiments on four challenging datasets for Re-ID tasks (including occluded person Re-ID and holistic person Re-ID) illustrate that our method can achieve SOTA performance.",
    "citation_count": "13",
    "year": "2023/03/01",
    "authors": [
        {
            "name": "Yunhua Lu",
            "country": ""
        },
        {
            "name": "Mingzi Jiang",
            "country": ""
        },
        {
            "name": "Zhi Liu",
            "country": ""
        },
        {
            "name": "Xinyu Mu",
            "country": ""
        }
    ],
    "keywords": []
}