{
    "url": "https://www.sciencedirect.com/science/article/pii/S0262885624001446",
    "title": "Exploring modality enhancement and compensation spaces for visible-infrared person re-identification",
    "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging task in  computer vision  due to the substantial modality gaps between visible and infrared images. The currently existing approaches can improve performance by addressing cross-modality discrepancies, but they often fail to generate compensation features that fully utilize the unique information present in each modality. Additionally, these methods mainly focus on pixel-level fusion of images, disregarding the challenge of modality misalignment. To address these issues, we propose a novel visible-infrared person re-identification method that explores modality enhancement and compensation spaces to extract more discriminative modality information. Furthermore, we introduce a modality mutual guidance strategy incorporating  identity information  mutual learning loss and modality-guided alignment loss, which can effectively leverage learned identity-related feature to guide alignment between visible and infrared modalities. Extensive experiments on public datasets demonstrate the significant superiority of our proposed method over existing state-of-the-art approaches.",
    "citation_count": "2",
    "year": "2024/06/01",
    "authors": [
        {
            "name": "Xu Cheng",
            "country": ""
        },
        {
            "name": "Shuya Deng",
            "country": ""
        },
        {
            "name": "Hao Yu",
            "country": ""
        }
    ],
    "keywords": []
}