{
    "url": "https://www.sciencedirect.com/science/article/pii/S0262885623000963",
    "title": "Adversarial anchor-guided feature refinement for adversarial defense",
    "abstract": "Adversarial training (AT), which is known as a robust training method for defending against adversarial examples, usually loses the performance of models for clean examples due to the feature distribution discrepancy between clean and adversarial. In this paper, we propose a novel Adversarial Anchor-guided Feature Refinement (AAFR) defense method aimed at reducing the discrepancy and delivering reliable performances for both clean and adversarial examples. We devise adversarial anchor that detects whether the feature comes from clean or adversarial example. Then, we use adversarial anchor to refine the feature to reduce the discrepancy. As a result, the proposed method substantially achieves adversarial robustness while preserving the performance for clean examples. The effectiveness of the proposed method is verified with comprehensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets.",
    "citation_count": "3",
    "year": "2023/08/01",
    "authors": [
        {
            "name": "Hakmin Lee",
            "country": ""
        },
        {
            "name": "Yong Man Ro",
            "country": ""
        }
    ],
    "keywords": []
}