{
    "url": "https://www.sciencedirect.com/science/article/pii/S026288562300001X",
    "title": "Conditional generative data-free knowledge distillation",
    "abstract": "Knowledge distillation  has made remarkable achievements in  model compression . However, most existing methods require the original  training data , which is usually unavailable due to privacy and security issues. This paper proposes a conditional generative data-free knowledge distillation (CGDD) framework for training lightweight networks without real data. This framework realizes efficient knowledge distillation based on conditional  image generation . Specifically, we treat the preset labels as ground truth to train a semi-supervised conditional generator. The trained generator can produce specified classes of training images. During training, we force the student model to extract the hidden knowledge in teacher feature maps, which provide crucial cues to the learning process. Meanwhile, we construct an  adversarial training  framework to promote distillation performance. The framework will help the student model to explore larger data space. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on different datasets. Compared with other data-free works, our method obtains state-of-the-art results on CIFAR100, Caltech101, and different versions of ImageNet datasets. The codes will be released.",
    "citation_count": "6",
    "year": "2023/03/01",
    "authors": [
        {
            "name": "Xinyi Yu",
            "country": ""
        },
        {
            "name": "Ling Yan",
            "country": ""
        },
        {
            "name": "Yang Yang",
            "country": ""
        },
        {
            "name": "Libo Zhou",
            "country": ""
        },
        {
            "name": "Linlin Ou",
            "country": ""
        }
    ],
    "keywords": []
}