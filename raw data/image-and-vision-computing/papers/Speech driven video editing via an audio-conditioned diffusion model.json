{
    "url": "https://www.sciencedirect.com/science/article/pii/S0262885624000143",
    "title": "Speech driven video editing via an audio-conditioned diffusion model",
    "abstract": "Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronised without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing. All code, datasets, and models used as part of this work are made publicly available here:  https://danbigioi.github.io/DiffusionVideoEditing/ .",
    "citation_count": "0",
    "year": "2024/02/01",
    "authors": [
        {
            "name": "Dan Bigioi",
            "country": ""
        },
        {
            "name": "Shubhajit Basak",
            "country": ""
        },
        {
            "name": "Michał Stypułkowski",
            "country": ""
        },
        {
            "name": "Maciej Zieba",
            "country": ""
        },
        {
            "name": "Hugh Jordan",
            "country": ""
        },
        {
            "name": "Rachel McDonnell",
            "country": ""
        },
        {
            "name": "Peter Corcoran",
            "country": ""
        }
    ],
    "keywords": []
}