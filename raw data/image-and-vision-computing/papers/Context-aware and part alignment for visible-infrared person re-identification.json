{
    "url": "https://www.sciencedirect.com/science/article/pii/S0262885623001658",
    "title": "Context-aware and part alignment for visible-infrared person re-identification",
    "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging problem of matching a person from visible and infrared modalities. Existing researches adopt the last  convolutional layer  features of the off-the-shelf  backbone network  as the representation to refine, which is unable to represent the heterogeneous cross-modality features with discriminant information. In this paper, we propose a novel graph-based aggregation learning network (GALNet) with visual Transformer embedding to mine both multi-layer features and part-level contextual cues for VI-ReID. We propose a novel feature memory module (FMM) to reserve global  discriminative features  of the low layers with the correlation modeling ability of the  graph convolution network  (GCN) which is supplemented to the final person representation. To learn more discriminant part-level features, an attentive part aggregation module (PAM) is designed to mine part relationships, leveraging the self-attention mechanism of the Transformer. By fusing these components, the global-level, and part-level discriminant information can be utilized. Extensive experiments on SYSU-MM01 and RegDB benchmarks demonstrate the effectiveness of our proposed methods compared with several state-of-the-art methods.",
    "citation_count": "11",
    "year": "2023/10/01",
    "authors": [
        {
            "name": "Jiaqi Zhao",
            "country": ""
        },
        {
            "name": "Hanzheng Wang",
            "country": ""
        },
        {
            "name": "Yong Zhou",
            "country": ""
        },
        {
            "name": "Rui Yao",
            "country": ""
        },
        {
            "name": "Lixu Zhang",
            "country": ""
        },
        {
            "name": "Abdulmotaleb El Saddik",
            "country": ""
        }
    ],
    "keywords": []
}