{
    "url": "https://www.sciencedirect.com/science/article/pii/S1383762121000515",
    "title": "Energy-efficient neural networks with near-threshold processors and hardware accelerators",
    "abstract": "Hardware for energy-efficient  AI  has received significant attention over the last years with both start-ups and large corporations creating products that compete at different levels of performance and  power consumption . The main objective of this hardware is to offer levels of efficiency and performance that cannot be obtained with general-purpose processors or  graphics processing units . In parallel, innovative hardware techniques such as near- and sub-threshold voltage processing have been revisited, capitalizing on the low-power requirements of deploying AI at the network edge. In this paper, we evaluate recent developments in hardware for energy-efficient AI, focusing on inference in embedded systems at the network edge. We then explore a heterogeneous configuration that deploys a  neural network  that processes multiple independent inputs and deploys convolutional and LSTM (Long Short-Term Memory) layers. This heterogeneous configuration uses two devices with different performance/power characteristics connected with a feedback loop. It obtains energy reductions measured at 75% while simultaneously maintaining the level of inference accuracy.",
    "citation_count": "5",
    "year": "2021/06/01",
    "authors": [
        {
            "name": "Jose Nunez-Yanez",
            "country": ""
        },
        {
            "name": "Neil Howard",
            "country": ""
        }
    ],
    "keywords": []
}