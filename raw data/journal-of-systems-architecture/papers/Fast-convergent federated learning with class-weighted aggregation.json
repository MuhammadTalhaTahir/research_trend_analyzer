{
    "url": "https://www.sciencedirect.com/science/article/pii/S138376212100093X",
    "title": "Fast-convergent federated learning with class-weighted aggregation",
    "abstract": "Recently,  federated learning  has attracted great attention due to its advantage of enabling model training in a distributed manner. Instead of uploading data for centralized training, it allows devices to keep local data private and only send parameters to server. Then the server aggregates local models to derive a global model. In this paper, we study the aggregation problem in federated learning, especially with non-independently and identically distributed data. Since existing scheme may degrade the representative of local models after aggregation, we propose to reallocate weights of local models based on contributions to each class. Then two class-weighted aggregation strategies are developed to improve the communication efficiency in federated learning. Evaluation shows that the proposed schemes reduce 30.49% and 23.59% of communication costs compared with FedAvg.",
    "citation_count": "35",
    "year": "2021/08/01",
    "authors": [
        {
            "name": "Zezhong Ma",
            "country": ""
        },
        {
            "name": "Mengying Zhao",
            "country": ""
        },
        {
            "name": "Xiaojun Cai",
            "country": ""
        },
        {
            "name": "Zhiping Jia",
            "country": ""
        }
    ],
    "keywords": []
}